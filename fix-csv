#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Perform some one-time fixes to the MacArthur Foundation 100&Change CSV file
#
# This is necessary because not everything can be addressed with the
# 'sanitize' script.  The 'sanitize' script isn't aware of CSV rows or
# columns; it just treats the whole CSV file as a normal text file.
# But for, e.g., fixing the YouTube links (see features.org for more),
# we really need a one-time transformation that knows what cell it's
# operating on.
#
# Copyright (C) 2017 Open Tech Strategies, LLC
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

##########################################################################
#                                                                        #
#   NOTE: This code is highly specific to the needs of the MacArthur     #
#   Foundation and is unlikely to be correct for your CSV.  It is        #
#   open source software, so please modify it to suit your needs.        #
#                                                                        #
##########################################################################

__doc__ = """\
Perform some one-time fixes to the MacArthur Foundation 100&Change CSV file.

Usage:

  $ fix-csv CSV_INPUT FIXED_CSV_OUTPUT

There are no options, because this is for a one-time transformation;
anything that would be an option should just be hardcoded in anyway.
"""

from bs4 import BeautifulSoup
import csv
import getopt
import html.parser
import re
import io
import sys
import warnings

def collapse_replace(string, old, new):
    "Return STRING, with OLD repeatedly replaced by NEW until no more OLD."
    while string.find(old) != -1: 
        string = string.replace(old, new)
    return string

def weaken_the_strong(html):
    """Strip any meaningless <strong>...</strong> tags from HTML.
    HTML is a Unicode string; the return value is either HTML or a new
    Unicode string based on HTML.

    If <strong> tags cover everything in HTML, remove the tags.  But if
    <strong> tags are used only sometimes, maybe they're meaningful, so
    leave them.  Basically, we want to make strength great again."""
    # If there's no strength here, move on.
    if not "<strong>" in html:
        return html
    
    # Remove the stuff inside strong tags
    soup = BeautifulSoup(html, "html.parser")
    while "<strong>" in str(soup):
        soup.strong.extract()

    # Check whether the non-bold stuff is more than just tags and
    # punctuation.  If not, all the important stuff was bold, so strip
    # bold and return.
    if re.sub(r"\W", "", soup.get_text()) == "":
        return re.sub("</?strong>", "", html)

    # OTOH, if the non-bold stuff contained letters or numbers, maybe
    # there's real content there, which means the html was a mix of
    # bold and non-bold text.  Better to leave it alone.
    return html

def form_well(html):
    """Return a well-formed version of HTML with dangling tags closed.
    Some of the input includes tables that cut off without closing
    tags; some entries leave <td> tags open too."""
    # Parse html and suppress warnings about urls in the text
    warnings.filterwarnings("ignore",
                            category=UserWarning, module='bs4')
    soup = BeautifulSoup(html, "html.parser")

    # Return well-formed html as a soup object
    return soup

def fix_csv(fname_in, fname_out=None, pare=None, show_table_problem=False):
    """Write a sanitized version of FNAME_IN (a CSV file) to FNAME_OUT.

    FNAME_IN is a filename.  FNAME_OUT is either a filename or None.
    If it is None, then write output to a returned StringIO object;
    otherwise, write output to the named file (and the return value is
    undefined).

    If PARE is not None, it is a positive integer indicating that only
    1 of every PARE entries should be processed, and the others skipped.

    If SHOW_TABLE_PROBLEM is not False, write debugging statements to
    sys.stderr to show entries that have unclosed <table> tags."""
    csv_reader = csv.reader(open(fname_in, 'rb'),
                            delimiter=',', quotechar='"')
    out = fname_out and open(fname_out, 'wb') or io.StringIO()
    csv_writer = csv.writer(out,
                            delimiter=',', quotechar='"', lineterminator="\n")
    header_row = next(csv_reader)
    csv_writer.writerow(header_row)

    # Convert "<foo>&nbsp;<bar>" and "</foo>&nbsp;<bar>" to
    # to "<foo> <bar>" and "</foo> <bar>" respectively.  This is
    # because those particular instances of "&nbsp;" in the data are
    # not very convincing, and they create noise when we're looking
    # for unnecessary escaping elsewhere.
    intertag_nbsp_re = re.compile('(?m)(</?[a-z]+>)&nbsp;(<[a-z]+>)')

    # Actually, some invalid identifiers would still match this,
    # because this regular expression doesn't check the length.
    # That's okay; we check length manually at the call site.
    youtube_id_re = re.compile('^[-_a-zA-Z0-9]+$')

    bullets_re = re.compile("^â€¢", re.MULTILINE)
    
    row_num = 0
    for row in csv_reader:
        row_num += 1  # first row here is row 1 (the header row was row 0) 
        if pare is not None and row_num % pare != 0:
            continue

        new_row = []
        org_name = None   # used only for debugging output
        cell_num = 0
        for cell in row:
            new_cell = cell.decode('utf-8')
            # A straight-up HTML-unescaping might be the right thing
            # (i.e., new_cell = html_parser.unescape(new_cell) below)
            # in the long run, but for now, let's do the same limited
            # set of unescapings the original 'sanitize' script did:
            new_cell = new_cell.replace('&amp;', '&')
            new_cell = new_cell.replace('&lt;', '<')
            new_cell = new_cell.replace('&gt;', '>')
            # The rest should be recursively collapsing replacements:
            new_cell = collapse_replace(new_cell, '&nbsp;&nbsp;', '&nbsp;')
            new_cell = collapse_replace(new_cell, '&nbsp; ', ' ')
            new_cell = collapse_replace(new_cell, ' &nbsp;', ' ')
            new_cell = collapse_replace(new_cell, '&nbsp;</', '</')
            new_cell = collapse_replace(new_cell, '\\"', '"')
            new_cell = re.sub(intertag_nbsp_re, '\\1 \\2', new_cell)
            if org_name is None:
                org_name = new_cell
            soup = form_well(new_cell)
            new_cell = weaken_the_strong(str(soup))
            
            # The parsing for lists requires an asterisk at the start
            # of the line, but some entries use bullets. This regex
            # swaps the bullets for asterisks.
            new_cell = bullets_re.sub("*", new_cell)

            if cell_num == 11:  # Some entries have "Select" as the topic
                # Column 12 (11 in 0-based) is "Primary Thematic Area",
                # and it's usually "Affordable and Clean Energy" or
                # "Sustainable Cities, Communities and Regions" or
                # something like that.  But in a few entries it's
                # blank or "Select", which must be handled specially.
                new_cell = new_cell.strip()
                if new_cell == "" or new_cell.lower() == "select":
                    # Cannot use special formatting here because this
                    # will be used as a category, not just as a value.
                    new_cell = "No Primary Thematic Area Selected"
            if (show_table_problem
                and new_cell.find('<table') != -1 
                and new_cell.find('</table') == -1):
                    sys.stderr.write("DEBUG: unclosed <table>: row %d, col %d (%s): '%s':\n'%s'\n\n" 
                                     % (row_num, cell_num, header_row[cell_num - 1], org_name, new_cell))
            if cell_num == 89:  # The pitch video cell may need fixing.
                # They all contain just an ID like "iIrGUi95ko4", not a URL
                # like "https://www.youtube.com/watch?v=iIrGUi95ko4".
                #
                # The rules for YouTube video identifiers seem to be:
                #
                #   * Exactly 11 characters long
                #   * Alphanumerics, "-", and "_" only (no spaces)
                #
                # There are a few special cases we handle below too.

                # "5m_6jsAwYNA " had trailing whitespace:
                new_cell = new_cell.strip()
                # "/JO7Eg6Kc-qk" kept their leading slash:
                if new_cell.find("/") == 0:
                    new_cell = new_cell[1:]
                if ((len(new_cell) == 11 and youtube_id_re.match(new_cell))
                    # One valid ID had "&feature" tacked on to the
                    # end, and two had time markers in the URL.
                    # Since we know they're valid, just say yes.
                    or new_cell.find("&feature") != -1
                    or new_cell.find("?t=") != -1):
                    # Okay, any of the above can become a video URL:
                    new_cell = "https://www.youtube.com/watch?v=" + new_cell
                else: # There were 8 with no video that we could find:
                    new_cell = ('<span style='
                            + '"color: red; font-family: monospace;" >'
                            + 'Could not convert "'
                            + new_cell
                            + '" to a YouTube video URL.'
                            + '</span>')
            new_row.append(new_cell.encode('utf-8'))
            cell_num += 1
        csv_writer.writerow(new_row)
    return out

def main():
    try:
        opts, args = getopt.getopt(sys.argv[1:], '',
                                   ["pare=", "show-table-problem"])
    except getopt.GetoptError as err:
        sys.stderr.write("ERROR: '%s'\n" % err)
        sys.exit(2)

    show_table_problem = False
    pare = None
    for o, a in opts:
        if o in ("--show-table-problem",):
            show_table_problem = True
        elif o=="--pare":
            pare = int(a)
        else:
            sys.stderr.write("ERROR: unrecognized option '%s'\n" % o)
            sys.exit(2)

    if len(args) != 2:
        sys.stderr.write(
            "ERROR: need CSV_INPUT and NEW_CSV_OUTPUT arguments.\n\n")
        sys.stderr.write(__doc__)
        sys.exit(1)
    fix_csv(args[0], args[1], pare, show_table_problem)    

if __name__ == '__main__':
    main()
