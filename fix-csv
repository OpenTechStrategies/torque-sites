#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Perform some one-time fixes to the MacArthur Foundation 100&Change CSV file
#
# This is necessary because not everything can be addressed with the
# 'sanitize' script.  The 'sanitize' script isn't aware of CSV rows or
# columns; it just treats the whole CSV file as a normal text file.
# But for, e.g., fixing the YouTube links (see features.org for more),
# we really need a one-time transformation that knows what cell it's
# operating on.
#
# Copyright (C) 2017 Open Tech Strategies, LLC
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

##########################################################################
#                                                                        #
#   NOTE: This code is highly specific to the needs of the MacArthur     #
#   Foundation and is unlikely to be correct for your CSV.  It is        #
#   open source software, so please modify it to suit your needs.        #
#                                                                        #
##########################################################################

__doc__ = """\
Perform some one-time fixes to the MacArthur Foundation 100&Change CSV file.

Usage:

  $ fix-csv CSV_INPUT FIXED_CSV_OUTPUT

There are no options, because this is for a one-time transformation;
anything that would be an option should just be hardcoded in anyway.
"""

import csv
import getopt
import re
import io
import sys
import warnings
from bs4 import BeautifulSoup

def collapse_replace(string, old, new):
    "Return STRING, with OLD repeatedly replaced by NEW until no more OLD."
    while string.find(old) != -1: 
        string = string.replace(old, new)
    return string

def weaken_the_strong(html):
    """Strip any meaningless <strong>...</strong> tags from HTML.
    HTML is a Unicode string; the return value is either HTML or a new
    Unicode string based on HTML.

    If <strong> tags cover everything in HTML, remove the tags.  But if
    <strong> tags are used only sometimes, maybe they're meaningful, so
    leave them.  Basically, we want to make strength great again."""
    # If there's no strength here, move on.
    if not "<strong>" in html:
        return html
    
    # Remove the stuff inside strong tags
    soup = BeautifulSoup(html, "html.parser")
    while "<strong>" in str(soup):
        soup.strong.extract()

    # Check whether the non-bold stuff is more than just tags and
    # punctuation.  If not, all the important stuff was bold, so strip
    # bold and return.
    if re.sub(r"\W", "", soup.get_text()) == "":
        return re.sub("</?strong>", "", html)

    # OTOH, if the non-bold stuff contained letters or numbers, maybe
    # there's real content there, which means the html was a mix of
    # bold and non-bold text.  Better to leave it alone.
    return html

def form_well(html):
    """Return a well-formed version of HTML with dangling tags closed.
    Some of the input includes tables that cut off without closing
    tags; some entries leave <td> tags open too."""
    # Parse html and suppress warnings about urls in the text
    warnings.filterwarnings("ignore",
                            category=UserWarning, module='bs4')
    soup = BeautifulSoup(html, "html.parser")

    # Return well-formed html as a soup object
    return soup

def fix_headers(header_row):
    """Update some of the column header strings in HEADER_ROW in place.
    HEADER_ROW must have exactly the 90 expected elements.
    There is no return value."""
    # NOTE: This duplicates functionality already present in csv2wiki,
    # and it means that we are basically maintaining the same column
    # rename map both here and in the csv2wiki-config.tmpl file.
    # However, the map here does much more error checking, since it
    # can, so we shouldn't try to combine the two maps.  (We could stop
    # maintaining the other map, and maybe we will, but this is useful
    # functionality in csv2wiki and we might as well continue testing
    # it every time we convert this particular spreadsheet to wiki.)

    # List of tuples, where the index of each tuple corresponds
    # to a column number, adjusted for 0-based indexing (list element
    # 0 corresponds to column 1).  The first element of each tuple is
    # the old column name, and the second element is either None,
    # meaning no change, or else is the new name.
    rename_map = [
        # Column 1:
        ("Registered Organization Name",
         "Organization",),
        # Column 2:
        ("Registration Location",
         "Location",),
        # Column 3:
        ("Legal Status",
         None,),
        # Column 4:
        ("Legal Status - Other",
         None,),
        # Column 5:
        ("Registration Location - Other",
         None,),
        # Column 6:
        ("City",
         None,),
        # Column 7:
        ("State / Province / Region",
         None,),
        # Column 8:
        ("Postal / Zip Code",
         "Postal Code",),
        # Column 9:
        ("Country",
         None,),
        # Column 10:
        ("Phone Number",
         None,),
        # Column 11:
        ("Area of Org Expertise",
         None,),
        # Column 12:
        ("Primary Thematic Area",
         "Topic",),
        # Column 13:
        ("Hear about 100 and Change Selection",
         None,),
        # Column 14:
        ("Hear about 100 and Change Notes",
         None,),
        # Column 15:
        ("Review_Number",
         "Review Number",),
        # Column 16:
        ("Executive Summary",
         None,),
        # Column 17:
        ("Team Purpose",
         None,),
        # Column 18:
        ("Legal Entity Name",
         None,),
        # Column 19:
        ("Principal Organization Point of Contact Name",
         "Principal Org Contact Name",),
        # Column 20:
        ("Principal Organization Point of Contact Title",
         "Principal Org Contact Title",),
        # Column 21:
        ("Principal Organization Point of Contact Phone",
         "Principal Org Contact Phone",),
        # Column 22:
        ("Principal Organization Point of Contact Email",
         "Principal Org Contact Email",),
        # Column 23:
        ("Principal Organization Registration Location",
         "Principal Org Registration Location",),
        # Column 24:
        ("Principal Organization Legal Status",
         "Principal Org Legal Status",),
        # Column 25:
        ("Principal Organization Legal Status - Other",
         "Principal Org Legal Status - Other",),
        # Column 26:
        ("Principal Organization Legal Status - Other - Notes",
         "Principal Org Legal Status - Other - Notes",),
        # Column 27:
        ("Principal Organization Jurisdiction",
         "Principal Org Jurisdiction",),
        # Column 28:
        ("Principal Organization Jurisdiction - Other",
         "Principal Org Jurisdiction - Other",),
        # Column 29:
        ("Principal Organization City",
         "Principal Org City",),
        # Column 30:
        ("Principal Organization State / Province",
         "Principal Org State / Province",),
        # Column 31:
        ("Principal Organization Postal Code",
         "Principal Org Postal Code",),
        # Column 32:
        ("Principal Organization Country",
         "Principal Org Country",),
        # Column 33:
        ("Principal Organization Primary Area of Expertise - Dropdown",
         "Principal Org Primary Area of Expertise - Dropdown",),
        # Column 34:
        ("Principal Organization Primary Area of Expertise - Notes",
         "Principal Org Primary Area of Expertise - Notes",),
        # Column 35:
        ("Principal Organization Primary Thematic Area of Proposed Solution - Dropdown",
         "Principal Org Primary Thematic Area of Proposed Solution - Dropdown",),
        # Column 36:
        ("Principal Organization Primary Thematic Area of Proposed Solution - Notes",
         "Principal Org Primary Thematic Area of Proposed Solution - Notes",),
        # Column 37:
        ("Team Structure",
         None,),
        # Column 38:
        ("Why Collaboration is Effective",
         None,),
        # Column 39:
        ("Problem Statement",
         None,),
        # Column 40:
        ("Proposed Solution",
         None,),
        # Column 41:
        ("Country/State 1",
         None,),
        # Column 42:
        ("Country/State 2",
         None,),
        # Column 43:
        ("Country/State 3",
         None,),
        # Column 44:
        ("Oceans/Space/Global",
         None,),
        # Column 45:
        ("Other Work Location",
         None,),
        # Column 46:
        ("Tactics and Technology",
         None,),
        # Column 47:
        ("Timeline and Key Milestones",
         None,),
        # Column 48:
        ("Evidence of Effectiveness",
         None,),
        # Column 49:
        ("Risk Assessment",
         None,),
        # Column 50:
        ("Plan Monitoring/Evaluation/Learning",
         None,),
        # Column 51:
        ("Previous Performance",
         None,),
        # Column 52:
        ("Organizational Capacity",
         None,),
        # Column 53:
        ("Budget Narrative",
         None,),
        # Column 54:
        ("Resource Requirements and Sustainability",
         None,),
        # Column 55:
        ("Other Considerations",
         None,),
        # Column 56:
        ("Charitable Purpose",
         None,),
        # Column 57:
        ("Private Benefit",
         None,),
        # Column 58:
        ("Lobbying Activities",
         None,),
        # Column 59:
        ("Human Subjects Research",
         None,),
        # Column 60:
        ("Team Story",
         None,),
        # Column 61:
        ("Total_Score",
         "Total Score",),
        # Column 62:
        ("Trait1_Score",
         "Meaningful?",),
        # Column 63:
        ("Trait2_Score",
         "Verifiable?",),
        # Column 64:
        ("Trait3_Score",
         "Feasible?",),
        # Column 65:
        ("Trait4_Score",
         "Durable?",),
        # Column 66:
        ("Trait1_Comment1",
         "Meaningful? - Comment 1",),
        # Column 67:
        ("Trait1_Comment2",
         "Meaningful? - Comment 2",),
        # Column 68:
        ("Trait1_Comment3",
         "Meaningful? - Comment 3",),
        # Column 69:
        ("Trait1_Comment4",
         "Meaningful? - Comment 4",),
        # Column 70:
        ("Trait1_Comment5",
         "Meaningful? - Comment 5",),
        # Column 71:
        ("Trait2_Comment1",
         "Verifiable? - Comment 1",),
        # Column 72:
        ("Trait2_Comment2",
         "Verifiable? - Comment 2",),
        # Column 73:
        ("Trait2_Comment3",
         "Verifiable? - Comment 3",),
        # Column 74:
        ("Trait2_Comment4",
         "Verifiable? - Comment 4",),
        # Column 75:
        ("Trait2_Comment5",
         "Verifiable? - Comment 5",),
        # Column 76:
        ("Trait3_Comment1",
         "Feasible? - Comment 1",),
        # Column 77:
        ("Trait3_Comment2",
         "Feasible? - Comment 2",),
        # Column 78:
        ("Trait3_Comment3",
         "Feasible? - Comment 3",),
        # Column 79:
        ("Trait3_Comment4",
         "Feasible? - Comment 4",),
        # Column 80:
        ("Trait3_Comment5",
         "Feasible? - Comment 5",),
        # Column 81:
        ("Trait4_Comment1",
         "Durable? - Comment 1",),
        # Column 82:
        ("Trait4_Comment2",
         "Durable? - Comment 2",),
        # Column 83:
        ("Trait4_Comment3",
         "Durable? - Comment 3",),
        # Column 84:
        ("Trait4_Comment4",
         "Durable? - Comment 4",),
        # Column 85:
        ("Trait4_Comment5",
         "Durable? - Comment 5",),
        # Column 86:
        ("Valid_Submission",
         "Valid Submission",),
        # Column 87:
        ("Big_Bet_Enduring_Commitment_Alignment",
         "Big Bet / Enduring Commitment Alignment",),
        # Column 88:
        ("Reason_For_Turndown",
         "Reason For Turndown",),
        # Column 89:
        ("Comments",
         None,),
        # Column 90:
        ("Pitch Video Link",
         None,),
        ]
    if len(header_row) != len(rename_map):
        sys.stderr.write("ERROR: unexpected number of columns (%d)\n"
                         % len(header_row))
        sys.exit(1)
    for i in range(0, 90):
        if rename_map[i][1] is not None:
            if header_row[i] == rename_map[i][0]:
                header_row[i] = rename_map[i][1]
            else:
                sys.stderr.write("ERROR: unexcepted column header:\n")
                sys.stderr.write("       expected \"%s\"\n" % rename_map[i][0])
                sys.stderr.write("       received \"%s\"\n" % header_row[i])
                sys.exit(1)
                

def fix_csv(fname_in, fname_out=None, pare=None, show_table_problem=False,
            reclassifications_csv=None):
    """Write a sanitized version of FNAME_IN (a CSV file) to FNAME_OUT.

    FNAME_IN is a filename.  FNAME_OUT is either a filename or None.
    If it is None, then write output to a returned StringIO object;
    otherwise, write output to the named file (and the return value is
    undefined).

    If PARE is not None, it is a positive integer indicating that only
    1 of every PARE entries should be processed, and the others skipped.

    If SHOW_TABLE_PROBLEM is not False, write debugging statements to
    sys.stderr to show entries that have unclosed <table> tags.

    If RECLASSIFICATIONS_CSV is not None, it is the path to a CSV file
    that contains reclassification information in column D (4) and an
    organization name in column A (1)."""
    reclass_csv_reader = None
    try:
        csv_reader = csv.reader(open(fname_in, encoding='utf-8'),
                                delimiter=',', quotechar='"')
        if reclassifications_csv is not None:
            reclass_csv_reader = csv.reader(open(reclassifications_csv,
                                                 encoding='utf-8'),
                                            delimiter=',', quotechar='"')
    except UnicodeDecodeError:
        sys.stderr.write("fix-csv expects utf-8-encoded unicode, not whatever is in this csv file.\n")
        sys.exit(-1)

    if fname_out:
        out = open(fname_out, 'w', encoding="utf-8")
    else:
        out = io.StringIO()
    csv_writer = csv.writer(out,
                            delimiter=',', quotechar='"', lineterminator="\n")
    header_row = next(csv_reader)
    fix_headers(header_row)
    csv_writer.writerow(header_row)

    # Keys are organization names; values are tuples of this form:
    #
    #   (original_classification, new_classification)
    #
    # In almost all cases, original_classification is "Other", but
    # there is at least one case where it's "Housing and Homelessness"
    # (the organization in that case is "WikiHouse Foundation").
    #
    # TODO: Using orgs as the key is problematic, because some orgs
    # appear more than once.  We could just toss them from the
    # dictionary, since we have no way of knowing which proposals the
    # different entries apply to.  It wouldn't even be enough to check
    # if maybe all the reclassifications for that org were to the same
    # category, because we still wouldn't know for sure that those are
    # the only proposals in the main spreadsheet submitted by that org
    # (though that's also true of all the reclassifications in general).
    reclassifications = {}
    if reclass_csv_reader is not None:
        ignored_headers = next(reclass_csv_reader)
        for row in reclass_csv_reader:
            reclassifications[row[0]] = (row[1], row[3],)

    # Convert "<foo>&nbsp;<bar>" and "</foo>&nbsp;<bar>" to
    # to "<foo> <bar>" and "</foo> <bar>" respectively.  This is
    # because those particular instances of "&nbsp;" in the data are
    # not very convincing, and they create noise when we're looking
    # for unnecessary escaping elsewhere.
    intertag_nbsp_re = re.compile('(?m)(</?[a-z]+>)&nbsp;(<[a-z]+>)')

    # Actually, some invalid identifiers would still match this,
    # because this regular expression doesn't check the length.
    # That's okay; we check length manually at the call site.
    youtube_id_re = re.compile('^[-_a-zA-Z0-9]+$')

    bullets_re = re.compile("^â€¢", re.MULTILINE)
    
    row_num = 0
    for row in csv_reader:
        row_num += 1  # first row here is row 1 (the header row was row 0) 
        if pare is not None and row_num % pare != 0:
            continue

        if row[0] in reclassifications:
            old, new = reclassifications[row[0]]
            # Topic ("Primary Thematic Area" is Column 12; we call it
            # 11 here because row is 0-based not 1-based.
            found_old_class = row[11].strip().lower()
            old = old.strip().lower()
            # sys.stderr.write("DEBUG: FOC: %s\n" % found_old_class)
            # sys.stderr.write("DEBUG: OLD: %s\n" % old)
            if (
                # Mostly old should be "other", except in one case:
                # for organization "WikiHouse Foundation", old has
                # a value of 'Housing and Homelessness'.  And it's
                # the only one matched, which makes me think that
                # maybe the reclassification sheet's claim that
                # "other" appears in the original spreadsheet is
                # bogus, and those proposals actually have some
                # random different classification.
                found_old_class == old
                or (found_old_class == "select" and old == "other")
                or (found_old_class == ""       and old == "other")):
                # TODO: data/ambiguous-reclassifications.org has a
                # list of all orgs that get found more than once by
                # this match.  But leave it for now, since it's
                # quite possible all this code will go away, given
                # how problematic the reclassification data is.
                # 
                # sys.stderr.write("DEBUG: for: '%s':\n" % row[0])
                # sys.stderr.write("       foc: '%s':\n" % found_old_class)
                # sys.stderr.write("       old: '%s':\n" % old)
                # sys.stderr.write("       new: '%s':\n" % new)
                pass

        new_row = []
        org_name = None   # used only for debugging output
        cell_num = 0
        for cell in row:
            new_cell = cell
            # A straight-up HTML-unescaping might be the right thing
            # (i.e., new_cell = html_parser.unescape(new_cell) below)
            # in the long run, but for now, let's do the same limited
            # set of unescapings the original 'sanitize' script did:
            new_cell = new_cell.replace('&amp;', '&')
            new_cell = new_cell.replace('&lt;', '<')
            new_cell = new_cell.replace('&gt;', '>')
            # The rest should be recursively collapsing replacements:
            new_cell = collapse_replace(new_cell, '&nbsp;&nbsp;', '&nbsp;')
            new_cell = collapse_replace(new_cell, '&nbsp; ', ' ')
            new_cell = collapse_replace(new_cell, ' &nbsp;', ' ')
            new_cell = collapse_replace(new_cell, '&nbsp;</', '</')
            new_cell = collapse_replace(new_cell, '\\"', '"')
            new_cell = re.sub(intertag_nbsp_re, '\\1 \\2', new_cell)
            if org_name is None:
                org_name = new_cell
            soup = form_well(new_cell)
            new_cell = weaken_the_strong(str(soup))
            
            # The parsing for lists requires an asterisk at the start
            # of the line, but some entries use bullets. This regex
            # swaps the bullets for asterisks.
            new_cell = bullets_re.sub("*", new_cell)

            if cell_num == 11:  # Some entries have "Select" as the topic
                # Column 12 (11 in 0-based) is "Primary Thematic Area",
                # and it's usually "Affordable and Clean Energy" or
                # "Sustainable Cities, Communities and Regions" or
                # something like that.  But in a few entries it's
                # blank or "Select", which must be handled specially.
                new_cell = new_cell.strip()
                if new_cell == "" or new_cell.lower() == "select":
                    # Cannot use special formatting here because this
                    # will be used as a category, not just as a value.
                    new_cell = "No Primary Thematic Area Selected"
            if (show_table_problem
                and new_cell.find('<table') != -1
                and new_cell.find('</table') == -1):
                sys.stderr.write("DEBUG: unclosed <table>: row %d, col %d (%s): '%s':\n'%s'\n\n"
                                 % (row_num, cell_num,
                                    header_row[cell_num - 1],
                                    org_name, new_cell))
            if cell_num == 89:  # The pitch video cell may need fixing.
                # They all contain just an ID like "iIrGUi95ko4", not a URL
                # like "https://www.youtube.com/watch?v=iIrGUi95ko4".
                #
                # The rules for YouTube video identifiers seem to be:
                #
                #   * Exactly 11 characters long
                #   * Alphanumerics, "-", and "_" only (no spaces)
                #
                # There are a few special cases we handle below too.

                # "5m_6jsAwYNA " had trailing whitespace:
                new_cell = new_cell.strip()
                # "/JO7Eg6Kc-qk" kept their leading slash:
                if new_cell.find("/") == 0:
                    new_cell = new_cell[1:]
                if ((len(new_cell) == 11 and youtube_id_re.match(new_cell))
                    # One valid ID had "&feature" tacked on to the
                    # end, and two had time markers in the URL.
                    # Since we know they're valid, just say yes.
                    or new_cell.find("&feature") != -1
                    or new_cell.find("?t=") != -1):
                    # Okay, any of the above can become a video URL:
                    video_id = new_cell
                    # Note that the "#ev" embed code below requires
                    # the EmbedVideo MediaWiki extension.  See issue
                    # #26 for details.
                    new_cell = "https://www.youtube.com/watch?v="   \
                              + video_id                            \
                              + "\n\n"                              \
                              + "{{#ev:youtube|" + video_id + "}}"
                else: # There were 8 with no video that we could find:
                    new_cell = ('<span style='
                            + '"color: red; font-family: monospace;" >'
                            + 'Could not convert "'
                            + new_cell
                            + '" to a YouTube video URL.'
                            + '</span>')
            new_row.append(new_cell)
            cell_num += 1
        csv_writer.writerow(new_row)
        print("Sanitized row %d." % row_num)
    return out

def main():
    """Sanitize the MacFound input and emit it as html-ized csv."""
    try:
        opts, args = getopt.getopt(sys.argv[1:], '',
                                   ["pare=",
                                    "reclassifications=",
                                    "show-table-problem"])
    except getopt.GetoptError as err:
        sys.stderr.write("ERROR: '%s'\n" % err)
        sys.exit(2)

    show_table_problem = False
    reclassifications_csv = None
    pare = None
    for o, a in opts:
        if o in ("--show-table-problem",):
            show_table_problem = True
        elif o in ("--reclassifications",):
            reclassifications_csv = a
        elif o == "--pare":
            pare = int(a)
        else:
            sys.stderr.write("ERROR: unrecognized option '%s'\n" % o)
            sys.exit(2)

    if len(args) != 2:
        sys.stderr.write(
            "ERROR: need CSV_INPUT and NEW_CSV_OUTPUT arguments.\n\n")
        sys.stderr.write(__doc__)
        sys.exit(1)
    fix_csv(args[0], args[1], pare, show_table_problem, 
            reclassifications_csv=reclassifications_csv)

if __name__ == '__main__':
    main()
